### Key concepts

AI-powered search combines information retrieval techniques with machine learning and deep learning to build search engines that continuously learn from users and content, moving beyond traditional "ten blue links" to domain-aware, contextual, conversational, multimodal, intelligent, and assistive experiences. The book covers the complete spectrum from semantic search using dense vector embeddings and knowledge graphs through retrieval augmented generation that tethers LLMs to actual information sources to prevent hallucinations, to machine-learned ranking models that automatically optimize relevance based on ongoing user interactions. Core technical approaches include reflected intelligence that uses user behavioral signals like clicks and purchases to crowdsource relevance judgments, content intelligence that extracts both explicit and implicit semantic knowledge graphs for nuanced query understanding, and personalized search leveraging collaborative filtering and vector embeddings for user-specific relevance. The architecture emphasizes two critical search steps of matching documents to queries and ranking by relevance, with extensive preprocessing for query understanding and postprocessing for answer extraction and summarization, all optimized through signals boosting, click models, active learning, and bias mitigation techniques.

### Who should read it and why

This book targets search engineers, software engineers, and data scientists building cutting-edge search engines who want hands-on experience integrating modern machine learning techniques, requiring Python programming skills and familiarity with SQL syntax for data aggregations. Product managers and business leaders seeking to understand AI-powered search possibilities and limitations will benefit from the comprehensive overview even without implementing techniques themselves, while readers wanting maximum value should follow Python code examples in Jupyter notebooks. The material assumes basic understanding of search engines like Elasticsearch, Solr, or OpenSearch, or vector databases, though this knowledge is helpful rather than required. It appeals to those recognizing that manual optimization of synonym lists, business rules, ontologies, and field weights over many years can be largely automated through machine learning, and to practitioners wanting plug-and-play support for popular search engines beyond the standardized Apache Solr examples. The book serves organizations integrating LLMs into search systems and developers building RAG implementations that provide accurate, up-to-date information as context for generative AI models.

### Practical applications

The book provides complete working examples in Docker containers with Jupyter notebooks enabling reproducible local execution with no additional configuration, using Python, PySpark, and Apache Solr with plug-and-play support for alternative engines and vector databases. Readers learn to collect and process user interaction signals to build signals-boosting models for important queries, implement collaborative filtering for personalized results, and train ranking classifiers on relevance judgments to create generalized ranking models. Practical applications include extracting knowledge graphs from content and signals for query expansion, classifying query intent and disambiguating term meanings using behavioral data, building complete query pipelines that parse domain-specific intent for semantic search, and implementing click models that generate implicit relevance feedback to continuously retrain ranking systems. Advanced techniques cover fine-tuning LLMs for question answering, implementing answer extraction from search results, optimizing vector search with ANN algorithms, quantization, and comparing bi-encoders versus cross-encoders for efficient semantic search. The material demonstrates multimodal foundation model integration for hybrid text-image-video queries, synthetic data generation, results summarization, and active learning with A/B testing to overcome ranking model bias. All code is open sourced under Apache 2.0 license with system requirements of 8GB RAM minimum and 25GB disk space for complete examples, supported by an active AI-Powered Search Community for ongoing updates and expert interaction.
